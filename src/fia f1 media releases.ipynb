{
 "metadata": {
  "name": "",
  "signature": "sha256:5c1de7249472b9f2042f91a9aa92de4d305cc2d8b9e6f7a53e58d0f0530acc2e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load in some libraries to handle the web page requests and the web page parsing...\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "#Note - I'm in Python3\n",
      "from urllib.parse import parse_qs"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!pip3 uninstall -y mechanize"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url='http://www.fia.com/sport/championships/news/formula-1-world-championship'\n",
      "response = requests.get(url,params={})"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "soup=BeautifulSoup(response.content)"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "events=soup.findAll('div',{'class':'past-event'})"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "urls=[]\n",
      "for event in events:\n",
      "    item=event.previousSibling\n",
      "    print(item.parent.attrs['href'])\n",
      "    urls.append(item.parent.attrs['href'])"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getPdfList(url):\n",
      "    response = requests.get(url,params={})\n",
      "    soup=BeautifulSoup(response.content)\n",
      "    links=soup.find_all('a')\n",
      "    pdflinks=[]\n",
      "    tmp=[]\n",
      "    filetypes=['pdf']\n",
      "    for l in links:\n",
      "        la=l.attrs['href']\n",
      "        for t in filetypes:\n",
      "            if t in la:\n",
      "                if la not in tmp:\n",
      "                    tmp.append(la)\n",
      "                    pdflinks.append((la,l.text.encode('utf-8')))\n",
      "    return pdflinks"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "scrapelist=[]\n",
      "for url in urls:\n",
      "    stub=url.split('2014/')[1].replace('2014-','').split('-')[0]\n",
      "    f=open(stub+'_2014_doclinks.txt','w')\n",
      "    writer = csv.writer(f)\n",
      "    links=[]\n",
      "    response = requests.get(\"http://fia.com\"+url,params={})\n",
      "    soup=BeautifulSoup(response.content)\n",
      "    x=soup.find('a',text  = re.compile('TIMING'))\n",
      "    try:\n",
      "        print(\"http://fia.com\"+x.attrs['href'])\n",
      "        links=links+getPdfList(\"http://fia.com\"+x.attrs['href'])\n",
      "    except:\n",
      "        print('..',url)\n",
      "        x=soup.find('a',text  = re.compile('EVENT INFORMATION'))\n",
      "        #print(\"http://fia.com\"+x.attrs['href'])\n",
      "        links=links+getPdfList(\"http://fia.com\"+x.attrs['href'])\n",
      "        #print(links)\n",
      "        response = requests.get(\"http://fia.com\"+x.attrs['href'],params={})\n",
      "        soup=BeautifulSoup(response.content)\n",
      "        for xx in ['Pre-Race','Practices','Qualifying','Race']:\n",
      "            xxx=soup.find('a',text  = re.compile('^'+xx+'$'))\n",
      "            #print(xxx)\n",
      "            links=links+getPdfList(\"http://fia.com\"+xxx.attrs['href'])\n",
      "    writer.writerows(links)\n",
      "    f.close()\n",
      "    for link in links:\n",
      "        scrapelist.append(\"http://fia.com\"+link[0])\n",
      "f=open('big_2014_doclinks.txt','w')\n",
      "f.write(\"\\n\".join(scrapelist))\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f=open('big_2014_doclinks.txt','w')\n",
      "f.write(\"\\n\".join(scrapelist))\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scrapelist"
     ],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {
      "activity": false
     },
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}